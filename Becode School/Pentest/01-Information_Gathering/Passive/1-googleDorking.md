# Google Dorking 🕵️‍♂️

## Learning Objectives 🎯

* Master the art of using Google query syntax
* Craft precise search requests
* Target sensitive information using Google queries

![Google Dorking GIF](https://www.eff.org/files/2021/10/29/floc-animation-1200.gif)

## What's the Buzz About? 🌐

Google, the juggernaut of search engines, has carved its place as the world's favorite. Despite concerns about privacy and data usage, it's a potent ally in our realm. Here's why:

### Some Eye-Opening Stats 👀

- It reigns with a whopping 90% of the search engine market share, except in Russia and China.

- Google indexes over 130,000 billion pages.

- The Googlebot crawls through a staggering 20 billion pages every day.

- It processes about 80,000 queries per second, totaling a staggering 6.9 billion daily and over 2,000 billion yearly.

These numbers tell the tale of Google's potential as a treasure trove of information.

Sources: [Google Trends](https://trends.google.fr/trends/?geo=BE) and [Stat Counter](https://gs.statcounter.com/)

### How to Tap into This Goldmine? 📡

Google dances to its own search syntax. This syntax can finely hone your searches, and that's where we're diving.

**First, Some Golden Rules**

- English Matters: The power of more content in English is real.

- Simplify: Stick to keywords; sentences are unnecessary.

- Operators are Friends: Utilize "", ., OR, AND, *, NOT, ~, ().

- Master the Prefixes: Get cozy with site:, filetype:, cache:, and more. Some are more effective than others, but combining them crafts a potent search.

- Image Search is Gold Too: Don't overlook image search's charm!

### What's Google Dorking? 🤖

It's all about the search possibilities that Google's syntax affords—a refined approach known as Google Dorking.

For instance, it can uncover sensitive info like emails, passwords, hidden files; locate authentication pages; scout internet-connected hardware (cameras, routers, printers); spot misconfigured web servers; zoom in on web server-related tools like phpmyadmin, phpinfo(); and sniff out vulnerabilities ripe for exploitation.

Feeling curious? [Here](https://www.exploit-db.com/google-hacking-database) is a glimpse of its capabilities!

## Ready to Learn? 🚀

Let's dive into an introduction:

[![Google Dorking Tutorial](https://i.ytimg.com/vi/hrVa_dhD-iA/maxresdefault.jpg)](https://www.youtube.com/watch?v=hrVa_dhD-iA)

## Practice Makes Perfect! 👨‍💻

Delve into the world of Google Dorking with this interactive  room @tryhackme:
* [Room Google Dorking](https://tryhackme.com/room/googledorking)

## Summary with questionaries from the interactive room:

Google, undoubtedly the superstar of "Search Engines," brings back memories of ancient platforms like Ask Jeeves. 🕵️‍♂️

However, let's not underestimate the sophistication beneath the surface of these search engines. There's a lot more happening behind the curtains than meets the eye. And guess what? We can exploit this complexity to uncover a treasure trove of information that simple wordlists can't offer. Research, especially in the realm of Cybersecurity, encompasses almost everything a pentester does. And here's the exciting part: MuirlandOracle has crafted an exceptional room that delves into the mindset required for effective research and the valuable insights it can yield.

"Search Engines" like Google aren't just big, they're massive indexers. They specialize in cataloging content strewn across the vast expanse of the World Wide Web.

These internet explorers rely on "Crawlers" or "Spiders" to scour the web for this content. I'll delve into these dynamic creatures in the next task. 🕷️

# 1. Understanding Web Crawlers 🕷️

Web crawlers are tools used by search engines to explore the internet and gather information from websites. They work in a couple of ways:

1. **Discovery by Visiting**: Crawlers start by visiting URLs, collecting data about the website's content type, and sending this information back to the search engine. They scrape various details from websites.

2. **URL Following**: Crawlers follow URLs found on previously crawled websites, spreading like a "virus" to discover more content.

## How Crawlers Work: Visualization 🌐

![Crawlers Visualization](https://i.imgur.com/4nrDDa0.png)

![Pears](https://i.imgur.com/nbbsAp4.png)

![crawler](https://i.imgur.com/CIM2c6N.png)

1. Crawler discovers "mywebsite.com", indexes its keywords ("Apple", "Banana", "Pear").
2. Search engine knows "mywebsite.com" has those keywords.
3. User searches for "Pear", "mywebsite.com" appears.
4. If "mywebsite.com" links to "anotherwebsite.com", the crawler explores it.
5. Crawler finds "anotherwebsite.com" keywords ("Tomatoes", "Strawberries", "Pineapples").
6. Search engine now knows about both domains and their keywords.

## How Crawlers Help: Recap 🔍

Crawlers help search engines gather website information to provide accurate search results. They:
- Discover content through URLs.
- Index keywords and data.
- Enable search engines to show relevant domains for user queries.

![recap](https://i.imgur.com/BJeI451.png)
## **Practice Time!** ✍️

1. **What is the primary way crawlers discover content?**
- [x] Through URLs
- [ ] By making direct requests to websites
- [ ] By sending emails to websites

2. **What does the term "crawling" refer to?**
- [ ] Finding hidden websites
- [ ] Collecting keywords from websites
- [x] Traversing and gathering data from websites

3. **What kind of information can crawlers gather from a website?**
- [ ] User reviews
- [ ] Colors and fonts
- [x] Keywords
- [ ] Server locations

4. **Name the key term of what a "Crawler" is used to do?**
    ...

5. **What is the name of the technique that "Search Engines" use to retrieve this information about websites?**
    ...


**Answers:**
1. Through URLs
2. Traversing and gathering data from websites
3. Keywords
4. index
5. crwaling

Remember, understanding crawlers helps you grasp how search engines find and present information! 🕵️‍♀️

# 2. Unveiling Search Engine Optimization (SEO) 🚀

Search Engine Optimization (SEO) is a buzzworthy topic in the realm of search engines. It's so important that entire businesses thrive by enhancing a domain's SEO "ranking". Essentially, search engines give more attention to domains that are easy to index. Think of it like a point-scoring game where several factors contribute.

## Factors Affecting SEO Points 📊

A few key influences on how these SEO points are awarded include:

- **Browser Responsiveness**: How well your website works on various browsers (Google Chrome, Firefox, Internet Explorer), including mobile phones.

- **Crawlability**: How easily your website can be crawled. "Sitemaps" help with this.

- **Keywords**: The words associated with your website. If someone searches for "Colours" and no site has that keyword, it won't show up.

Search engines use complex algorithms to rank websites, but the exact details aren't shared openly.

## Boosting Your Domain's Visibility 🔍

Want your domain to shine? You can pay to improve your ranking through advertising. Search engines' business side kicks in here.

There are various online tools - sometimes provided by the search engine providers themselves that will show you just how optimised your domain is. For example, let's use [Google's Site Analyser](https://web.dev/) to check the rating of [TryHackMe](https://tryhackme.com/):

![analyzer tools](https://i.imgur.com/kvaFolh.png)
![tools](https://i.imgur.com/6rFnpVc.png)

According to this tool, TryHackMe has an SEO rating of **85/100** (as of 14/11/2020). That's not too bad and it'll show the justifications as to how this score was calculated below on the page.
## Who Controls the Crawlers? 🕵️‍♂️

Apart from search engines providing the crawlers, website owners have a say. They decide what content the crawlers can scrape. But here's the twist – not all parts of a website should be indexed. Imagine a secret admin login page showing up on Google! Not cool.

## **Practice Time!** ✍️

**Which of these is NOT a factor affecting SEO points?**
- [ ] Browser responsiveness
- [ ] Keywords
- [x] Moon phase
- [ ] Crawlability

**What does SEO stand for?**
- [ ] Social Engineering Outreach
- [x] Search Engine Optimization
- [ ] Super Exciting Online

**Why do businesses invest in improving SEO?**
- [ ] To play games
- [x] To enhance website performance
- [ ] To create secret login pages
- [ ] To get more followers on social media

**Who decides what content can be scraped by crawlers?**
- [ ] Aliens
- [ ] Search engines only
- [x] Website owners
- [ ] Your neighbor's cat

**Answers:**
1. Moon phase
2. Search Engine Optimization
3. To enhance website performance
4. Website owners

Understanding SEO helps websites get noticed and ensures search engines provide reliable results! 🌟

# 3. Decoding Robots.txt 🤖

**Robots.txt** is like a bouncer for your website, controlling what search engine **crawlers** can access. This text file, usually found at the root directory, sets rules for what these crawlers can and cannot do. It's a way to tell them, "Hey, you can enter here, but stay out of these rooms!"

## Unveiling the Keywords 🗝️

Here are the main players in the Robots.txt scene:

- **User-agent**: Specifies which type of crawler can enter. You can use `*` as a wildcard.
- **Allow**: Grants access to specific directories or files.
- **Disallow**: Denies access to certain directories or files.
- **Sitemap**: Points to your sitemap for better indexing (we'll dive into sitemaps later).

## Basic Example 📜

Let's break down a simple Robots.txt:

```plaintext
User-agent: *
Allow: /
Sitemap: http://mywebsite.com/sitemap.xml
```

- All crawlers can access the site.
- They're allowed to index everything.
- The sitemap is at `http://mywebsite.com/sitemap.xml`.

## Securing Secrets 🔒

Need to hide stuff from crawlers? Robots.txt works as a "blacklist". Anything not mentioned is fair game. Take a look:

```plaintext
User-agent: *
Disallow: /super-secret-directory/
Disallow: /not-a-secret/but-this-is/
Sitemap: http://mywebsite.com/sitemap.xml
```

- All crawlers can roam, but...
- Stay away from `/super-secret-directory/`.
- Don't explore `/but-this-is/` (sub-directory included).
- The sitemap is at `http://mywebsite.com/sitemap.xml`.

## Exclusive Access 🕵️‍♂️

Now, let's play favorites:

```plaintext
User-agent: Googlebot
Allow: /
User-agent: msnbot
Disallow: /
Sitemap: http://mywebsite.com/sitemap.xml
```

- Googlebot has VIP access to everything.
- msnbot is denied entry completely.
- The sitemap remains at `http://mywebsite.com/sitemap.xml`.

## Keeping Secrets Safe 🤫

Protect sensitive files using regex magic:

```plaintext
User-agent: *
Allow: /
Disallow: /*.ini$
Sitemap: http://mywebsite.com/sitemap.xml
```

- All can access the site.
- But they can't touch any `.ini` file, anywhere.
- Sitemap still resides at `http://mywebsite.com/sitemap.xml`.

Why hide `.ini` files? They often contain secret configuration data. Think of other file formats that hold confidential stuff!

## **Practice Time!** 📝

1. **What is the purpose of Robots.txt?**
- [ ] It controls website themes
- [ ] It helps with server configuration
- [x] It sets rules for search engine crawlers
- [ ] It generates sitemaps

2. **Which keyword specifies directories or files that crawlers can access?**
- [ ] Disallow
- [ ] User-agent
- [x] Allow
- [ ] Sitemap

3. **What does "Disallow: /secret-stash/" in Robots.txt mean?**
- [x] Crawlers can access everything except `/secret-stash/`
- [ ] Crawlers can access only `/secret-stash/`
- [ ] Crawlers can access everything, including `/secret-stash/`
- [ ] Crawlers can access `/secret-stash/` after 6 PM

4. **What's the purpose of the "Sitemap" keyword in Robots.txt?**
- [ ] It tells crawlers what music to play
- [ ] It's a secret command for advanced crawling
- [ ] It points to the website's style guide
- [x] It specifies the location of the sitemap for better indexing

5. **Where would "robots.txt" be located on the domain "**ablog.com**"**
	ablog.com/robots.txt

6. **If a website was to have a sitemap, where would that be located?**
	/sitemap.xml

7. **How would we only allow "Bingbot" to index the website?**
	user-agent: Bingbot

8. **How would we prevent a "Crawler" from indexing the directory "/dont-index-me/"?**
	Dissallow: /dont-index-me/

9. **What is the extension of a Unix/Linux system configuration file that we might want to hide from "Crawlers"?**
	.conf

**Answers:**
1. It sets rules for search engine crawlers
2. Allow
3. Crawlers can access everything except `/secret-stash/`
4. It specifies the location of the sitemap for better indexing
5. ablog.com/robots.txt
6. /sitemap.xml
7. user-agent: Bingbot
8. Dissallow: /dont-index-me/
9. .conf

Understanding Robots.txt keeps your website's secrets safe and guides crawlers in exploring your content responsibly! 🕵️‍♀️🕵️‍♂️



Time to embrace your inner digital detective! 🕶️

![Investigate](https://media.giphy.com/media/UvI75iAc9jhLCRD0pn/giphy.gif)

## [Next >>](./2-social.md)